{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Introduction to NLP course (2017-2018).\n",
    "\n",
    "Notebook 6: Co-occurrence matrices. DISSECT.\n",
    "\n",
    "by Venelin Kovatchev, University of Barcelona\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import section\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will explore a simple co-occurrence Distributional Semantic Model.\n",
    "\n",
    "It is based on surface co-occurrence within a window of 3 in the gutenberg corpus (see also Notebook 3).\n",
    "\n",
    "First, we obtain the co-occurrence statistics from the corpus. Then we convert and save them in format that is native to the DISSECT library. We use the DISSECT library to load the files, generate a co-occurrence matrix and perform simple operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Load the corpus\n",
    "corpus = gutenberg.words()\n",
    "\n",
    "## generate the raw co-occurrence count within a window of 3\n",
    "cooc = BigramCollocationFinder.from_words(corpus,window_size=4).ngram_fd.items()\n",
    "\n",
    "## convert the list of collocates in a dictionary\n",
    "\n",
    "# Initialize the dict\n",
    "cooc_dict = {}\n",
    "\n",
    "# Loop through the list\n",
    "for pair,freq in cooc:\n",
    "    # Check and initialie the variables\n",
    "    word1,word2 = pair\n",
    "    # Check if entries for the words exist\n",
    "    # If not, create them\n",
    "    if word1 not in cooc_dict:\n",
    "        cooc_dict[word1]={}\n",
    "        \n",
    "    if word2 not in cooc_dict:\n",
    "        cooc_dict[word2]={}\n",
    "        \n",
    "    # Check if entries for the particular combination exists\n",
    "    # If not, initialize them\n",
    "    if word2 not in cooc_dict[word1]:\n",
    "        cooc_dict[word1][word2]=0\n",
    "    if word1 not in cooc_dict[word2]:\n",
    "        cooc_dict[word2][word1]=0\n",
    "    # Update the dict variables\n",
    "    cooc_dict[word1][word2]+=freq\n",
    "    cooc_dict[word2][word1]+=freq\n",
    "\n",
    "## Generate the row, col and data variables for the DISSECT\n",
    "\n",
    "# Initialize the variables\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "\n",
    "# Loop through the dictionary\n",
    "for word_1 in cooc_dict:\n",
    "    # Add an entry to the rows variable\n",
    "    # there should be no duplications, but we check anyway\n",
    "    if word_1 not in rows:\n",
    "        rows.append(word_1)\n",
    "    # Loop through the entries in the dict\n",
    "    for word_2 in cooc_dict[word_1]:\n",
    "        # Add an entry in the cols, if it's not already added\n",
    "        if word_2 not in cols:\n",
    "            cols.append(word_2)\n",
    "        # Add the value to the data\n",
    "        data.append(word_1 + \" \" + word_2 + \" \" + str(cooc_dict[word_1][word_2]))\n",
    "        \n",
    "## Output the row,col,data to files\n",
    "\n",
    "# Define the base name\n",
    "fname = \"gutenberg_surface_3\"\n",
    "\n",
    "# Generate tuples of fname data for the files\n",
    "out = []\n",
    "out.append((fname + \".rows\",rows))\n",
    "out.append((fname + \".cols\",cols))\n",
    "out.append((fname + \".sm\",data))\n",
    "\n",
    "# Loop through the out var\n",
    "for (filename,content) in out:\n",
    "    # Open the file\n",
    "    with open(filename,\"w\") as out_file:\n",
    "        # Loop through the rows variable\n",
    "        for entry in content:\n",
    "            # Remove non unicode chars\n",
    "            entry = entry.encode('utf8', 'replace')\n",
    "            # Write the entry\n",
    "            out_file.write(entry)\n",
    "            # Add newline\n",
    "            out_file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import section for dissect\n",
    "from composes.semantic_space.space import Space\n",
    "from composes.utils import io_utils\n",
    "from composes.transformation.scaling.ppmi_weighting import PpmiWeighting\n",
    "from composes.transformation.dim_reduction.svd import Svd\n",
    "from composes.similarity.cos import CosSimilarity\n",
    "from composes.utils import scoring_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the three input files and creating the raw space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress...1000000\n",
      "Progress...2000000\n",
      "Progress...3000000\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder where the data files are\n",
    "my_path = \"\"\n",
    "\n",
    "# Loading the matrix from the three different files\n",
    "my_space = Space.build(data = my_path + \"gutenberg_surface_3.sm\",\n",
    "                       rows = my_path + \"gutenberg_surface_3.rows\",\n",
    "                       cols = my_path + \"gutenberg_surface_3.cols\",\n",
    "                       format = \"sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Transform the space using PMI and SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Transforming the semantic space using PPMI\n",
    "my_ppmi_space = my_space.apply(PpmiWeighting())\n",
    "\n",
    "# Reducing dimensions\n",
    "my_svd_space = my_space.apply(Svd(50))\n",
    "my_ppmi_svd_space = my_ppmi_space.apply(Svd(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Basic operations with the co-occurrence vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcilating similarity between man and woman\n",
      "PPMI and SVD matrix 0.783663262385\n",
      "Obtaining the 5 most similar words to 'car'\n",
      "PPMI and SVD matrix\n",
      "[('car', 1.0), ('stick', 0.88739669849128044), ('window', 0.88371037927544349), ('lawn', 0.8766521928187585), ('corner', 0.8755986796085643)]\n",
      "Comparing similarity with 'gold standard'\n",
      "Pairs: [('awful', 'terrible'), ('awful', 'great'), ('awful', 'fast'), ('chop', 'cut'), ('chop', 'bake'), ('chop', 'smile'), ('material', 'fabric'), ('material', 'car'), ('material', 'stone')]\n",
      "Gold scores ['10', '8', '5', '10', '7', '2', '10', '3', '7']\n",
      "\n",
      " PPMI and SVD matrix:\n",
      "Predicted scores [0.85, 0.48, 0.49, 0.42, 0.33, 0.21, 0.31, 0.32, 0.33]\n",
      "Spearman correlation: 0.418864620531\n",
      "Pearson correlation: 0.518846556776\n"
     ]
    }
   ],
   "source": [
    "# Comparing similarity between \"man\" and \"woman\"\n",
    "print \"Calcilating similarity between man and woman\"\n",
    "print \"PPMI and SVD matrix\",my_ppmi_svd_space.get_sim(\"man\", \"woman\", CosSimilarity())\n",
    "\n",
    "# Comparing the 5 most similar words to \"car\"\n",
    "print \"Obtaining the 5 most similar words to 'car'\"\n",
    "print \"PPMI and SVD matrix\\n\",my_ppmi_svd_space.get_neighbours(\"car\", 5, CosSimilarity())\n",
    "\n",
    "# Comparing the similarity with \"gold standard\"\n",
    "print \"Comparing similarity with 'gold standard'\"\n",
    "fname = my_path + \"synonyms.txt\"\n",
    "# Load the pairs\n",
    "word_pairs = io_utils.read_tuple_list(fname, fields=[0,1])\n",
    "# Load the score\n",
    "gold = io_utils.read_list(fname, field=2)\n",
    "# Predict similarity\n",
    "predicted_ppmi_svd = [round(sim,2) for sim in my_ppmi_svd_space.get_sims(word_pairs, CosSimilarity())]\n",
    "print \"Pairs:\",word_pairs\n",
    "print \"Gold scores\",gold\n",
    "print \"\\n PPMI and SVD matrix:\"\n",
    "print \"Predicted scores\",predicted_ppmi_svd\n",
    "print \"Spearman correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"spearman\")\n",
    "print \"Pearson correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"pearson\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
